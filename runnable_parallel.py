from langchain_openai import ChatOpenAI  # Import OpenAI's LLM interface
from langchain_core.prompts import PromptTemplate  # Used to define input prompts
from langchain_core.output_parsers import StrOutputParser  # Parses LLM output into plain string
from dotenv import load_dotenv  # Loads API keys and env variables from .env file
from langchain.schema.runnable import RunnableSequence, RunnableParallel  # Chain and parallel execution components

load_dotenv()  # Load environment variables (like OpenAI API key)

# Prompt to generate a tweet based on the topic
prompt1 = PromptTemplate(
    template='Generate a tweet about {topic}',  # Template for tweet generation
    input_variables=['topic']  # Expects a 'topic' input
)

# Prompt to generate a LinkedIn post based on the topic
prompt2 = PromptTemplate(
    template='Generate a Linkedin post about {topic}',  # Template for LinkedIn post
    input_variables=['topic']  # Expects the same 'topic' input
)

model = ChatOpenAI()  # Initialize the OpenAI chat model (e.g., GPT-4)

parser = StrOutputParser()  # Parser to convert model response to plain string

# Run both chains (tweet + LinkedIn post) in parallel using the same topic input
parallel_chain = RunnableParallel({
    'tweet': RunnableSequence(prompt1, model, parser),      # Chain for generating tweet
    'linkedin': RunnableSequence(prompt2, model, parser)    # Chain for generating LinkedIn post
})

# Invoke the parallel chain with topic 'AI'
result = parallel_chain.invoke({'topic': 'AI'})

# Print the tweet generated by the model
print(result['tweet'])

# Print the LinkedIn post generated by the model
print(result['linkedin'])

# Flow Summary:
# Input: {'topic': 'AI'}

# Runs two chains in parallel:

# One to generate a tweet

# One to generate a LinkedIn post

# Returns both results as a dictionary:

# python
# Copy
# Edit
